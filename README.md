# Resampling for Classification vs. Class Probability Prediction

Oversampling and undersampling techniques are often used in machine learning classification tasks on imbalanced data. The primary motivation is for refined calibration and performance. Many times classification accuracy alone will over-state the quality of a particular forecast when minority class occurrence is very rare (< 1%). A model might achieve 99% accuracy but incorrectly classify every single minority outcome simply because 99% of observations are in the majority class. False positive and false negative rates, also called Type I and II errors, provide a more nuanced view of predictive accuracy. Resampling methods help to adjust for imbalanced outcomes by artificially changing the proportion of classes in data to improve minority class performance. The confusion matrix along with specificity and sensitivity rates are then used to monitor classification improvements across each class.
